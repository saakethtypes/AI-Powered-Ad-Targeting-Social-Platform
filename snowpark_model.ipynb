{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\College\\7374\\FinalProject\\env\\lib\\site-packages\\snowflake\\connector\\options.py:107: UserWarning: You have an incompatible version of 'pyarrow' installed (11.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "Failed to import ArrowResult. No Apache Arrow result set format can be used. ImportError: DLL load failed while importing arrow_iterator: The specified procedure could not be found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.session.Session at 0x1dab0c1bb20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import json\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.session.Session at 0x1dab0c1bb20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PutResult(source='download.png', target='download.png', source_size=4111, target_size=4112, source_compression='NONE', target_compression='NONE', status='UPLOADED', message='')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# session.sql('create or replace stage dash_udfs').collect()\n",
    "session.file.put(\"../temp/download.png\",'@dash_models',overwrite=True,auto_compress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\"FILE_NAME\": [\"image1\"], \"OUTPUT\": [\"cat\"]})\n",
    "# s = session.sql(\"session.sql('create database if not exists IMAGES')\")\n",
    "# s\n",
    "# session.write_pandas(df, \"IMAGES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PutResult(source='image_1.jpeg', target='image_1.jpeg', source_size=26237, target_size=26240, source_compression='NONE', target_compression='NONE', status='UPLOADED', message='')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.file.put('../temp/image_1.jpeg','@dash_models',overwrite=True,auto_compress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Model Directory\n",
    "directory = '../model/'\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        # Import trained model\n",
    "        # session.file.put(f,'@dash_models',overwrite=True,auto_compress=False)\n",
    "        session.add_import('@dash_models/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PutResult(source='image.jpg', target='image.jpg', source_size=85949, target_size=85952, source_compression='NONE', target_compression='NONE', status='UPLOADED', message='')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.file.put(\"../backend/images/image.jpg\",'@dash_models',overwrite=True,auto_compress=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit\n",
    "import os\n",
    "from snowflake.snowpark.types import  StringType, IntegerType\n",
    "\n",
    "# session.add_import('@dash_models/m/tokenizer.json')\n",
    "# session.add_import('@dash_models/pytorch_model.bin')\n",
    "# session.add_import('@dash_files/mobilenetv3.py')\n",
    "session.add_import('@dash_models/image.jpg')\n",
    "@udf(name='image_caption_generator',session=session,replace=True,is_permanent=True,stage_location='@dash_models')\n",
    "def image_caption_generator() -> str:\n",
    "  from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "  import torch\n",
    "  import sys\n",
    "  from PIL import Image\n",
    "  from io import BytesIO\n",
    "  import requests\n",
    "  import glob \n",
    "  IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "  import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "  # return os.listdir(import_dir)\n",
    "  \n",
    "  model_path = import_dir\n",
    "\n",
    "  model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "  feature_extractor = ViTFeatureExtractor.from_pretrained(model_path)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "  image_file_name = \"/image.jpg\"\n",
    "  device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "  max_length = 16\n",
    "  num_beams = 4\n",
    "  gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "  images = []\n",
    "  i_image = Image.open(model_path+image_file_name)\n",
    "  if i_image.mode != \"RGB\":\n",
    "    i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "  images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  # os.remove(model_path+\"downloa\")\n",
    "  return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_caption(image_link : str) -> str:\n",
    "    # Snowpark imports \n",
    "    from snowflake.snowpark.session import Session\n",
    "    import json\n",
    "    connection_parameters = json.load(open('connection.json'))\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    # ... continued in backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_links_in_str = [\"https://as2.ftcdn.net/v2/jpg/03/03/62/45/1000_F_303624505_u0bFT1Rnoj8CMUSs8wMCwoKlnWlh5Jiq.jpg\"]\n",
    "predicted_label = session.sql('''SELECT image_caption_generator()''').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wooden table topped with a wooden table cloth'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package transformers in the local environment is 4.28.1, which does not fit the criteria for the requirement transformers. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package pillow in the local environment is 9.5.0, which does not fit the criteria for the requirement pillow. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "session.add_packages([\"transformers\",\"Pillow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createdAt': '2023-04-25 19:54:28', 'sk': 'user#username1', 'pk': 'user#username1', 'interests': []}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://4t6zjqbvra77cljo42ody4xtmi0smplq.lambda-url.us-east-1.on.aws/'\n",
    "data = {'method': 'LOGIN_USER', 'userpk': 'user#'+\"username1\"}\n",
    "res = requests.post(url, json=data)\n",
    "res = res.json()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_link': 'https://admmedia.s3.ap-south-1.amazonaws.com/user_uploads/img_0390164a-4946-469e-ae1a-b4cbff3e8dbb', 'sk': 'img_0390164a-4946-469e-ae1a-b4cbff3e8dbb', 'pk': 'post', 'recognitions': []}, {'image_link': 'https://admmedia.s3.ap-south-1.amazonaws.com/user_uploads/img_581c53dd-7c99-4ce0-b737-c34987ff8550', 'sk': 'img_581c53dd-7c99-4ce0-b737-c34987ff8550', 'pk': 'post', 'recognitions': []}, {'generated_interested': ['cat', 'guitar'], 'image_link': 'https://admmedia.s3.ap-south-1.amazonaws.com/user_uploads/exsample.png', 'sk': 'post#2', 'pk': 'post'}, {'generated_interested': ['cat', 'guitar'], 'image_link': 'https://admmedia.s3.ap-south-1.amazonaws.com/user_uploads/exsample.png', 'sk': 'post#3', 'pk': 'post'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://4t6zjqbvra77cljo42ody4xtmi0smplq.lambda-url.us-east-1.on.aws/'\n",
    "data = {'method': 'LIST_POSTS'}\n",
    "res = requests.post(url, json=data)\n",
    "res = res.json()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "# Function to download and save image locally\n",
    "\n",
    "nltk.data.path.append('../nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['brown', 'fox'], 'quick brown fox was running')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_nouns(caption):\n",
    "    text = caption\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    nouns = [word for word, tag in tagged_words if tag in ['NN', 'NNS']]\n",
    "    return nouns, caption\n",
    "extract_nouns(\"quick brown fox was running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch product details\n",
    "def fetch_product_details(product_name):\n",
    "    # URL of the Amazon search page for the given product\n",
    "    url = f\"https://www.amazon.com/s?k={product_name}&ref=nb_sb_noss_1\"\n",
    "    headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # Send an HTTP GET request to the Amazon search page\n",
    "    response = requests.get(url,headers=headers)\n",
    "    print(response)\n",
    "    # Parse the HTML content of the Amazon search page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the first 3 product links on the search page\n",
    "    product_links = soup.find_all(\"a\", {\"class\": \"a-section\"})\n",
    "    print(product_links)\n",
    "    # Fetch the product details for each product link\n",
    "    for link in product_links[:3]:\n",
    "        # Extract the product title\n",
    "        title = link.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"}).text.strip()\n",
    "        print(title)\n",
    "        # Extract the product price\n",
    "        price = link.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "        if price is None:\n",
    "            price = link.find(\"span\", {\"class\": \"a-offscreen\"})\n",
    "        if price is not None:\n",
    "            price = price.text.strip()\n",
    "\n",
    "        # Extract the product image URL\n",
    "        image_url = link.find(\"img\")[\"src\"]\n",
    "\n",
    "        # Extract the product link\n",
    "        product_url = \"https://www.amazon.com\" + link[\"href\"]\n",
    "\n",
    "        # Print the product details\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Price: {price}\")\n",
    "        print(f\"Image URL: {image_url}\")\n",
    "        print(f\"Product URL: {product_url}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Example usage\n",
    "fetch_product_details(\"laptop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [503]>\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\t\n",
    "\ttry:\n",
    "\t\t# Outer Tag Object\n",
    "\t\ttitle = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "\n",
    "\t\t# Inner NavigatableString Object\n",
    "\t\ttitle_value = title.string\n",
    "\n",
    "\t\t# Title as a string value\n",
    "\t\ttitle_string = title_value.strip()\n",
    "\n",
    "\t\t# # Printing types of values for efficient understanding\n",
    "\t\t# print(type(title))\n",
    "\t\t# print(type(title_value))\n",
    "\t\t# print(type(title_string))\n",
    "\t\t# print()\n",
    "\n",
    "\texcept AttributeError:\n",
    "\t\ttitle_string = \"\"\t\n",
    "\n",
    "\treturn title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "\ttry:\n",
    "\t\tprice = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\n",
    "\texcept AttributeError:\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t# If there is some deal price\n",
    "\t\t\tprice = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\n",
    "\t\texcept:\t\t\n",
    "\t\t\tprice = \"\"\t\n",
    "\n",
    "\treturn price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "\ttry:\n",
    "\t\trating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "\t\t\n",
    "\texcept AttributeError:\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\trating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "\t\texcept:\n",
    "\t\t\trating = \"\"\t\n",
    "\n",
    "\treturn rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "\ttry:\n",
    "\t\timage_element = soup.find('img', {'id': 'landingImage'})\n",
    "\t\timage_url = image_element['data-old-hires']\n",
    "\t\tprint(f\"Product Image URL: {image_url}\")\n",
    "\t\treturn image_url\n",
    "\texcept AttributeError:\n",
    "\t\timage_url = \"\"\t\n",
    "\n",
    "\treturn image_url\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "\ttry:\n",
    "\t\tavailable = soup.find(\"div\", attrs={'id':'availability'})\n",
    "\t\tavailable = available.find(\"span\").string.strip()\n",
    "\n",
    "\texcept AttributeError:\n",
    "\t\tavailable = \"Not Available\"\t\n",
    "\n",
    "\treturn available\t\n",
    "\n",
    "\n",
    "def get_ads(product_name):\n",
    "\n",
    "\t# Headers for request\n",
    "\tHEADERS = ({'User-Agent':\n",
    "\t            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "\t            'Accept-Language': 'en-US'})\n",
    "\n",
    "\t# The webpage URL\n",
    "\tURL = f\"https://www.amazon.com/s?k={product_name}&ref=nb_sb_noss_2\"\n",
    "\t\n",
    "\t# HTTP Request\n",
    "\twebpage = requests.get(URL, headers=HEADERS)\n",
    "\tprint(webpage)\n",
    "\t# Soup Object containing all data\n",
    "\tsoup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "\t# Fetch links as List of Tag Objects\n",
    "\tlinks = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})[:3]\n",
    "\tprint(links)\n",
    "\t# Store the links\n",
    "\tlinks_list = []\n",
    "\n",
    "\t# Loop for extracting links from Tag Objects\n",
    "\tfor link in links:\n",
    "\t\tprint(link)\n",
    "\t\tlinks_list.append(link.get('href'))\n",
    "\n",
    "\tads = []\n",
    "\t# Loop for extracting product details from each link \n",
    "\tfor link in links_list:\n",
    "\n",
    "\t\tnew_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
    "\n",
    "\t\tnew_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\t\t\n",
    "\t\t# Function calls to display all necessary product information\n",
    "\t\tads.append({\n",
    "\t\t\t\"title\": get_title(new_soup),\n",
    "\t\t\t\"image_url\": get_review_count(new_soup),\n",
    "\t\t\t\"rating\": get_rating(new_soup),\n",
    "\t\t\t\"link\": \"https://www.amazon.com\"+ link\n",
    "\t\t})\n",
    "\treturn ads\n",
    "get_ads(\"laptop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [502]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "api_url = 'https://4t6zjqbvra77cljo42ody4xtmi0smplq.lambda-url.us-east-1.on.aws/'\n",
    "\n",
    "data = {'method': 'POST_RECOGNITION', 'postsk':'img_326e9c11-4936-4ba3-b4e0-47cf0da771eb.jpg', 'caption':\"tssssssss\", \"ad_links\": [\"s\",\"a\",\"a\"], \"recommends\":[\"k\",\"e\",\"t\",\"h\"]}\n",
    "res = requests.post(api_url, json=data)\n",
    "res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif (event['method'] == \"POST_RECOGNITION\"):\n",
    "        bucket_name = 'admmedia'\n",
    "        s3_client = boto3.client('s3')\n",
    "        key = {'pk': {'S': 'post'}, 'sk': {'S': event['postsk']}}\n",
    "\n",
    "        # specify the attributes to update\n",
    "        update_expression = 'SET #attr1 = :rec, #attr2 = :cap, #attr3 = :ad_links '\n",
    "        expression_attribute_names = {'#attr1': 'recommends', '#attr2': 'caption' , '#attr3': 'ad_links' }\n",
    "        expression_attribute_values = {':rec': {'S': event[\"recommends\"]}, ':cap': {'N': event[\"caption\"]}, ':links': {'N': event[\"ad_links\"]}}\n",
    "        \n",
    "        # update the item in the table\n",
    "        response = dynamodb.update_item(\n",
    "            TableName=table_name,\n",
    "            Key=key,\n",
    "            UpdateExpression=update_expression,\n",
    "            ExpressionAttributeNames=expression_attribute_names,\n",
    "            ExpressionAttributeValues=expression_attribute_values,\n",
    "        )\n",
    "        return {\n",
    "            'statusCode': response['ResponseMetadata']['HTTPStatusCode'],\n",
    "            'body': \"post_updated\",\n",
    "                \"headers\": {\n",
    "            'Content-Type': 'text/html',\n",
    "        }\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
